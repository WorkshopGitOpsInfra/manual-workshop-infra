= Deploy Managed Cluster Setup

[#managedconfiguration]
== Managed configuration

Checkout to your working branch. 

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
git branch # main
git checkout sno-<name>-setup
----

Argo hub setup has been already performed, so you just need to focused on managed your single node cluster.

As every user is deploying its own bootstrap file, they must have an unique name and they should be deployed on your scoped project where your user has privileges.

Then you will have to replace <name> by your cluster and your repo url to your github account.

Make sure after cluster-definition, the following folder (sno-<name>) is named as your assigned cluster as this value will be used as a parameter for ApplicationSet.
Then update cluster.json values.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
{
  "environment": "dev",
  "cloud": "aws",
  "project": "project-sno-<name>",
  "region": "<region>",
  "cluster": {
    "name": "sno-<name>",
    "address": "https://api.sno-<name>.<domain>.opentlc.com:6443"
  }
}
----

You must also update managed-setup-a for your own managed cluster. Note how this application deploys to a different namespace and argo instance compared to hub-setup application:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: managed-setup-a-<name>
  namespace: openshift-operators
spec:
  destination:
    namespace: openshift-operators
    server: https://kubernetes.default.svc
  project: project-sno-<name>
  source:
    repoURL: https://github.com/<your_user>/workshop-gitops-content-deploy.git
    targetRevision: sno-<name>-setup
    path: cluster-addons/cluster-addons-as/
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
----      

The source path of this Application points to the https://argo-cd.readthedocs.io/en/stable/user-guide/application-set/[ApplicationSet] used. 
ApplicationSet is a CR that can be uset to genereate Applications to multiple cluster from multiple originis. ApplicationSet controller is installed alongside Argo CD 
even though it must be explicitaly enabled. 

ApplicationSet is a helpful approach for multi cluster management, and it supplements Argo CD by adding additional features in support of cluster-administrator-focused scanerios. Some of those are:

- Ability to use a single Kubernetes manifest to target multiple Kubernetes clusters with Argo CD.

- Ability to use a single Kubernetes manifest to deploy multiple applications from one or multiple Git repositories with Argo CD.

- Improved support for monorepos (multiple Argo CD Application resources defined within a single Git repository).

- Improves ability of individual cluster tenants to deploy applications using ArgoCD (without needing to involve privileged cluster admin in enabling the destanion servers).

ApplicationSet uses generators to create Application with multiple origins and destinations using templating. 

https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/Generators/[Generator] used may vary depending on your use case, in this example
we are using Git Files generator so cluster definition values are used as parameters. In case we had N cluster-definition files, the ApplicationSet would generate N Applications automatically.

As we did before, we should update ApplicationSet name with your lab data.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: gitops-setup-sno-<name>
  namespace: openshift-operators
spec:
  generators:
  - git:
      repoURL: https://github.com/romerobu/workshop-gitops-content-deploy.git
      revision: sno-<name>-setup
      files:
      - path: "cluster-definition/**/cluster.json"
  template:
    metadata:
      name: 'gitops-setup-{{cluster.name}}-a'
    spec:
      project: '{{project}}'
      source:
        repoURL: https://github.com/<your_user>/workshop-gitops-content-deploy.git
        targetRevision: sno-<name>-setup
        path: cluster-addons/charts/gitops-setup 
      destination:
        server: '{{cluster.address}}'
      syncPolicy:
        automated:
          prune: true
          selfHeal: true       
----    

This first ApplicationSet deploys the operator and an initial non default cluster wide instance for day 2 and infra operations.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: bootstrap-sno-<name>
  namespace: openshift-operators
spec:
  generators:
  - git:
      repoURL: https://github.com/romerobu/workshop-gitops-content-deploy.git
      revision: sno-<name>-setup
      files:
      - path: "cluster-definition/**/cluster.json"
  template:
    metadata:
      name: 'bootstrap-{{cluster.name}}-a'
    spec:
      project: '{{project}}'
      source:
        repoURL: https://github.com/<your_user>/workshop-gitops-content-deploy.git
        targetRevision: sno-<name>-setup
        path: cluster-addons/charts/bootstrap-app
      destination:
        server: '{{cluster.address}}'
      syncPolicy:
        automated:
          prune: true
          selfHeal: true       
----     

Then update bootstrap-app values.yaml file with your cluster data too:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
clusters:
  sno-<name>:
    applicationNamespace: openshift-gitops
    namespace: ''
    destination: 'https://kubernetes.default.svc'
    project: default
    code:
      repo: https://github.com/<your_user>/workshop-gitops-content-deploy.git
      path: cluster-addons/charts/bootstrap
      target: sno-<name>-setup
----

And finally replace values in bootstrap values.yaml file;

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
...
vault: 
  vault_addr: "http://vault-vault.apps.argo-hub.<domain>.opentlc.com"
  avp_type: vault
...
----

This ApplicationSet deploys an Application on the recently deployed instance on destination cluster to deploy and manage a second instance for applications.

Then navigate under source path to take a look to the Helm charts used for deploying GitOps and setting up the initial config for managed clusters.

*Reminder*: ApplicationSet controller is not enabled by default and must be configured on argocd instance.

[#helmcharts]
== Helm charts

First Helm chart is *gitops-setup*, which deploys GitOps operator on managed clusters. This chart is intented to deploy any kind of operator, even though in this case we are
only deploying GitOps operator.

If you navigate to subscription.yaml resource you will see there is a global value for applying env variables for GitOps. 

These config values disable the default argocd instance and enables a new instance to be cluster wide. This means this argo application controller sa will have
permissions to work in all namespaces within the cluster.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: {{ $key }}
  namespace: {{ $val.namespace }}
  {{- if $.Values.global.argocd.enabled }}
  annotations:
    argocd.argoproj.io/sync-wave: "-5"
  {{- end }}
spec:
  channel: {{ $val.channel }}
  installPlanApproval: {{ $val.approval }}
  name: {{ $val.name }}
  source: redhat-operators
  sourceNamespace: openshift-marketplace
{{- if $val.csv }}
  startingCSV: {{ $val.csv }}
{{- end }}
{{- if $.Values.operators.gitops.enabled }}
  config:
    env:
    - name: ARGOCD_CLUSTER_CONFIG_NAMESPACES
      value: openshift-gitops # namespace of argocd instance
    - name: DISABLE_DEFAULT_ARGOCD_INSTANCE
      value: "true"        
----   

By default any new instance created is namespace scoped, this means you will only be allowed to deploy within your namespace. If you want to deploy across all namespace
you need to change this configuration to make the instance cluster wide. Additionally your argo sa may not have privileges enough to work with cluster wide resources and you might need to 
assign a new role binding for it.

You can either create a custom role binding or labelling any managed namespace by argo so it will create this role binding automatically only for that namespace.

After setting this global var you can see a new cluster role binding for this sa and this configuration on argocd console.

image::cluster-wide-role-binding.png[]

You can take a look to global env vars https://developers.redhat.com/articles/2023/03/06/5-global-environment-variables-provided-openshift-gitops#5_environment_variables__overview[here], how to label namespaces https://docs.openshift.com/container-platform/4.10/cicd/gitops/setting-up-argocd-instance.html#gitops-deploy-resources-different-namespaces_setting-up-argocd-instance[here] 
and how to create a role binding https://docs.openshift.com/container-platform/4.12/cicd/gitops/configuring-an-openshift-cluster-by-deploying-an-application-with-cluster-configurations.html#gitops-additional-permissions-for-cluster-config_configuring-an-openshift-cluster-by-deploying-an-application-with-cluster-configurations[here].

Once the operator is running, we need to deploy ArgoCD instance. To make sure instance is deployed after the operator is running we use sync waves and custom resources healthcheck.

Sync waves are defined on each resource as annotations, and they tell argo the order in which resources should be applied once the previous is already in healthy status.
You can take a look in detail to the https://argo-cd.readthedocs.io/en/stable/user-guide/sync-waves/[documentation].

For some specific resources they need a custom healthcheck. Most of the objects only require existing to work but others like subscription may exists but not progress to a successful status so we
need a https://argo-cd.readthedocs.io/en/stable/operator-manual/health/[custom healthcheck] to make sure the next sync wave does not start till the operators are properly installed.

Resource healthcheck is defined in the argo instace, which is also deployed using helm charts.

Navigate to ArgoCD instance and take a look to the *resourceCustomizations* section, as well as other configurations that we will review later on.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
kind: ArgoCD
apiVersion: argoproj.io/v1alpha1
metadata:
  name: {{ $.Values.argocd.name }}
  namespace: {{ $.Values.argocd.controller }}
  {{- if $.Values.argocd.enabled }}
  annotations:
    argocd.argoproj.io/sync-wave: "-10"
  {{- end }}  
spec:
  sso:
    dex:  
      openShiftOAuth: true
      resources:
        limits:
          cpu: 500m
          memory: 256Mi
        requests:
          cpu: 250m 
          memory: 128Mi
    provider: dex
  resourceTrackingMethod: annotation+label
  applicationSet:
    logLevel: info
  controller:
    logLevel: debug
    resources:
      limits:
        cpu: 2000m
        memory: 2048Mi
      requests:
        cpu: 250m
        memory: 1024Mi
  ha:
    enabled: false
    resources:
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 250m
        memory: 128Mi
  rbac:
    defaultPolicy: ''
    policy: |-
      g, system:cluster-admins, role:admin
      g, cluster-admins, role:admin     
    scopes: '[groups]'
  redis:
    resources:
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 250m
        memory: 128Mi
  repo:
    resources:
      limits:
        cpu: 1000m
        memory: 1024Mi
      requests:
        cpu: 250m
        memory: 256Mi                                          
  resourceExclusions: "- apiGroups:\n  - tekton.dev\n  clusters:\n  - '*'\n  kinds:\n  - TaskRun\n  - PipelineRun        \n"
  server:
    resources:
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 125m
        memory: 128Mi
    route:
      enabled: true
  resourceCustomizations: |
    operators.coreos.com/Subscription:
      health.lua: |      
        health_status = {}
        if obj.status ~= nil then
          if obj.status.conditions ~= nil then
            numDegraded = 0
            numPending = 0
            msg = ""
            for i, condition in pairs(obj.status.conditions) do
              msg = msg .. i .. ": " .. condition.type .. " | " .. condition.status .. "\n"
              if condition.type == "InstallPlanPending" and condition.status == "True" then
                numPending = numPending + 1
              elseif (condition.type == "InstallPlanMissing" and condition.reason ~= "ReferencedInstallPlanNotFound") then
                numDegraded = numDegraded + 1
              elseif (condition.type == "CatalogSourcesUnhealthy" or condition.type == "InstallPlanFailed" or condition.type == "ResolutionFailed") and condition.status == "True" then
                numDegraded = numDegraded + 1
              end
            end
            if numDegraded == 0 and numPending == 0 then
              health_status.status = "Healthy"
              health_status.message = msg
              return health_status
            elseif numPending > 0 and numDegraded == 0 then
              health_status.status = "Progressing"
              health_status.message = "An install plan for a subscription is pending installation"
              return health_status
            else
              health_status.status = "Degraded"
              health_status.message = msg
              return health_status
            end
          end
        end
        health_status.status = "Progressing"
        health_status.message = "An install plan for a subscription is pending installation"
        return health_status
----   

This first instance *argocd-infra* is intended for day 2 and infra operations and namespace creation and management.

Next chart is *bootstrap-app*. This chart deploys an Application on argocd-infra instance to apply *bootstrap* chart.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
{{- range $key, $val := $.Values.clusters }}
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: {{ $key }}-bootstrap
  namespace: {{ $val.applicationNamespace }}
spec:
  destination:
    server: {{ $val.destination }}
    namespace: ''
  project: {{ $val.project }}
  source:
    helm:
      valueFiles:
        - values.yaml
    path: {{ $val.code.path }}
    repoURL: {{ $val.code.repo }}
    targetRevision: {{ $val.code.target }}
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
{{- end }}      
----

Then on *bootstrap* folder you can find resources for deploying a second *argocd-apps* instance, namespaces, vault and rbac configuration.

Argo instance is slightly similar to the first one but it has some special customization, lets take a look:

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
kind: ArgoCD
apiVersion: argoproj.io/v1alpha1
metadata:
  name: {{ $.Values.argocd.name }}
  namespace: {{ $.Values.operators.gitops.namespace }}
  {{- if $.Values.global.argocd.enabled }}
  annotations:
    argocd.argoproj.io/sync-wave: "5"
  {{- end }}  
spec:
  sso:
    dex:  
      openShiftOAuth: true # 1
      resources:
        limits:
          cpu: 500m
          memory: 256Mi
        requests:
          cpu: 250m 
          memory: 128Mi
    provider: dex
  resourceTrackingMethod: annotation+label # 2
  applicationSet: # 3
    logLevel: info
  controller:
    resources:
      limits:
        cpu: 2000m
        memory: 2048Mi
      requests:
        cpu: 250m
        memory: 1024Mi
  ha:
    enabled: false
    resources:
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 250m
        memory: 128Mi
  rbac: # 4
    defaultPolicy: ''
    policy: |-
      g, {{ $.Values.argocd.group }}, role:admin
      p, role:operator, applications, get, */*, allow
      p, role:operator, applications, sync, */*, allow
      g, argo-admins, role:admin 
      g, argo-readers, role:readonly
      g, argo-operators, role:operator
      g, argo-dev-operators, role:operator-dev      
    scopes: '[groups]'
  redis:
    resources:
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 250m
        memory: 128Mi
  repo: 
    resources:
      limits:
        cpu: 1000m
        memory: 1024Mi
      requests:
        cpu: 250m
        memory: 256Mi
    env:
        - name: AVP_AUTH_TYPE
          valueFrom:
            secretKeyRef:
              key: AVP_AUTH_TYPE
              name: argocd-vault-plugin-credentials
        - name: AVP_TYPE
          valueFrom:
            secretKeyRef:
              key: AVP_TYPE
              name: argocd-vault-plugin-credentials
        - name: VAULT_ADDR
          valueFrom:
            secretKeyRef:
              key: VAULT_ADDR
              name: argocd-vault-plugin-credentials
        - name: AVP_K8S_ROLE
          valueFrom:
            secretKeyRef:
              key: AVP_K8S_ROLE
              name: argocd-vault-plugin-credentials       
    mountsatoken: true
    sidecarContainers: # 5
      - command:
          - /var/run/argocd/argocd-cmp-server
        image: 'quay.io/argoproj/argocd:v2.4.8'
        name: avp-helm
        volumeMounts:
          - mountPath: /var/run/argocd
            name: var-files
          - mountPath: /home/argocd/cmp-server/plugins
            name: plugins
          - mountPath: /tmp
            name: tmp-dir
          - mountPath: /home/argocd/cmp-server/config
            name: cmp-plugin
          - mountPath: /usr/local/bin/argocd-vault-plugin
            name: custom-tools
            subPath: argocd-vault-plugin
    volumeMounts:
      - mountPath: /usr/local/bin/argocd-vault-plugin
        name: custom-tools
        subPath: argocd-vault-plugin
    volumes:
      - configMap:
          name: cmp-plugin
        name: cmp-plugin
      - emptyDir: {}
        name: custom-tools
      - emptyDir: {}
        name: tmp-dir                  
    initContainers:
      - args:
          - >-
            wget -O /custom-tools/argocd-vault-plugin
            https://github.com/argoproj-labs/argocd-vault-plugin/releases/download/v${AVP_VERSION}/argocd-vault-plugin_${AVP_VERSION}_linux_amd64
            && chmod +x /custom-tools/argocd-vault-plugin && ls -la
            /custom-tools/
        command:
          - sh
          - '-c'
        env:
          - name: AVP_VERSION
            value: 1.11.0
        image: 'alpine:3.8'
        name: download-tools
        volumeMounts:
          - mountPath: /custom-tools
            name: custom-tools               
  resourceExclusions: "- apiGroups:\n  - tekton.dev\n  clusters:\n  - '*'\n  kinds:\n  - TaskRun\n  - PipelineRun        \n"
  server:
    resources:
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 125m
        memory: 128Mi
    route:
      enabled: true
  configManagementPlugins: | # 6
    - name: argocd-vault-plugin
      generate:
        command: ["argocd-vault-plugin"]
        args: ["generate", "./"]      
  resourceCustomizations: | # 7
    operators.coreos.com/Subscription:
      health.lua: |      
        health_status = {}
        if obj.status ~= nil then
          if obj.status.conditions ~= nil then
            numDegraded = 0
            numPending = 0
            msg = ""
            for i, condition in pairs(obj.status.conditions) do
              msg = msg .. i .. ": " .. condition.type .. " | " .. condition.status .. "\n"
              if condition.type == "InstallPlanPending" and condition.status == "True" then
                numPending = numPending + 1
              elseif (condition.type == "InstallPlanMissing" and condition.reason ~= "ReferencedInstallPlanNotFound") then
                numDegraded = numDegraded + 1
              elseif (condition.type == "CatalogSourcesUnhealthy" or condition.type == "InstallPlanFailed" or condition.type == "ResolutionFailed") and condition.status == "True" then
                numDegraded = numDegraded + 1
              end
            end
            if numDegraded == 0 and numPending == 0 then
              health_status.status = "Healthy"
              health_status.message = msg
              return health_status
            elseif numPending > 0 and numDegraded == 0 then
              health_status.status = "Progressing"
              health_status.message = "An install plan for a subscription is pending installation"
              return health_status
            else
              health_status.status = "Degraded"
              health_status.message = msg
              return health_status
            end
          end
        end
        health_status.status = "Progressing"
        health_status.message = "An install plan for a subscription is pending installation"
        return health_status   
----

1. Dex uses groups and users defined within Openshift by checking the Oauth server

2. Overrides default tracking method by label to annotation+label

3. Enable ApplicationSet controller

4. Configure argo RBAC

5. Configure vault plugin as a sidecar container

6. Configure new plugin for vault

7. Configure resource healthcheck for Subscription

As you may notice this instance contains some parametes for configuring vault plugin (which we will discuss later) and rbac model.

Rbac is defined on *rbac* folder and includes the basic configuration for argo RBAC and projects.

The https://argo-cd.readthedocs.io/en/stable/operator-manual/rbac/[RBAC] feature enables restriction of access to Argo CD resources. Argo CD does not have its own user management system and has only one built-in user admin. 
The admin user is a superuser and it has unrestricted access to the system. RBAC requires SSO configuration or one or more local users setup. 
Once SSO or local users are configured, additional RBAC roles can be defined, and SSO groups or local users can then be mapped to roles.

Argo CD has two pre-defined roles but RBAC configuration allows defining roles and groups (see below).

- role:readonly - read-only access to all resources

- role:admin - unrestricted access to all resources

Additionally to the defined roles, it is possible to create some specific roles to allow argo-operators and argo-dev-operators group members manage applications in Argo CD.

Then if you navigate to rbac folder you can see a Group and a Role binding resource to give cluster-admin permissions on argo to the admin user configured via Htpasswd.

For RBAC we need to differentiate between global configuration on argocd intance and projects RBAC.

If you navigate to rbac section on argo instance, you will see some rbac policies starting like *g*  and *p*.

Policies starting with g assign roles to openshift local groups (they can be both argo roles and ocp roles) and their users, while policies starting with p define specific policies for projects, resources, projects and applications and their operations.

The following sections collect the information around Argo CD Roles and Argo CD permission in the managed clusters. It is important to understand the functionality matrix and permission that the following sections try to implement:

- argo-admins: group members have full permissions in Argo CD to admin

- argo-readers: group members have read-only permissions in Argo CD to access all information

- argo-operators: group members have permission to manage applications (get and sync) only in Argo CD

- argo-dev-operators: group members have permission to manage applications (get and sync) only in Argo CD dev project

- apimanager01: user has no permissions to see anything in Argo CD but has permissions to create objects in the Openshift Clusters

Then on AppProject we can define restrictions like source repo, destination servers and resource whitelist allowed per project. Moreover you can define local roles for that AppProject.

Last but not least are *namespaces*. Namespaces are created as part of the bootstrap process by the argo infra instance so the operator in charge of managing apps lifecycle does not 
need to have cluster wide privileges. (Add link to some article about namespaces management)

Finally push your changes to your working branch, login to argocd instance (argo hub cluster) with user-<name> and deploy the bootstrap Application.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
git add .
git commit -m "Your message"
git push origin sno-<name>-setup
----

To create bootstrap application, navigate to Argo console, click on **New app**, then **Edit as Yaml**, **save** and finally **Create**.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: managed-setup-a-<name>
  namespace: openshift-operators
spec:
  destination:
    namespace: openshift-operators
    server: https://kubernetes.default.svc
  project: project-sno-<name>
  source:
    repoURL: https://github.com/<your_user>/workshop-gitops-content-deploy.git
    targetRevision: sno-<name>-setup
    path: cluster-addons/cluster-addons-as/
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
----

At this point you should see some applications on syncing on your argo console. You cannot see your colleagues deployments thanks to RBAC.

image::managed-setup.png[]

Deep dive on *managed-setup-a-<name>* to check all the resources created. Next go back to the initial view and see how the Applications rendered by ApplicationSet are created.

image::managed-setup-a-name.png[]

Then navigate to argo hub console using your user with view role, navigate to ArgoCD instance (Installed Operators -> Openshift GitOps -> ArgoCD), take a look to global rbac policies and then navigate to your AppProject
to verify yor local permissions.

If you try to deploy a new Application from the console you will see you cannot deploy to a different cluster destination than yours.

image::clusters-list.png[]

It happens the same with projects, you can only see yours:

image::projects-list.png[]

Once this is completed login to you managed cluster, and verify:

- GitOps operator is installed.

- Argo-infra instance exists and is cluster wide .

image::cluster-wide.png[]

- Argo-apps instance exists and is not cluster wide .

- Existing Dev and Pro AppProject on argocd-apps instance.

- Login as admin and verify you can create apps on dev project (argocd-apps).

  Go to this https://github.com/romerobu/helm-infra-gitops-workshop.git[repo] and checkout to working branch, then push it.

[.lines_7]
[.console-input]
[source, shell,subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  namespace: openshift-operators
  name: sno-<name>-vault
spec:
  destination:
    namespace: vault-secrets
    server: 'https://kubernetes.default.svc'
  source:
    helm:
      parameters:
        - name: vault.enabled
          value: 'true'
    path: .
    repoURL: 'https://github.com/<your_user>/helm-infra-gitops-workshop.git'
    targetRevision: sno-<name>
  project: dev
  syncPolicy:
    automated:
      prune: false
      selfHeal: false  
----

- Login as user user04 (argo-dev-operators) with role operator-dev and verify you can get and sync apps on dev project.

- Login as user apimanager01 (api-manager) and verify you do not have permissions to see apps on dev project.